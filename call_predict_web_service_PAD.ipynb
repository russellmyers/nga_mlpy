{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Call Azure ML for PY Framework web service - Payroll Anomaly Detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import requests\n",
    "from ngamlfpy.utils import pulled_df_to_json \n",
    "from ngamlfpy.hrxmlconfig import MLModelConfig\n",
    "from ngamlfpy.pipeline import FileFinder\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "web_service_url = \"http://b03d85f2-b070-497e-a0f8-03ec2315c0af.westus.azurecontainer.io/score\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ml_service = 'PAD'\n",
    "model_code =  'A001' \n",
    "model_version = '001' \n",
    "use_csv_input = True\n",
    "base_folder = './data'\n",
    "input_file_name= 'PAD_A001_001_WDY_ALT_TST_ALTBM_WDT_000_sm_202003_input.csv'  #<== csv input file to use for predictions\n",
    "num_emps_to_clip = 10  # <== Restrict to this number of employees to use from input file. (-1 = use all emps in file) \n",
    "save_csv_file_as_json = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find input file in relevant data folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "finder = FileFinder(ml_service, use_model_name_in=True, use_model_name_out=False, model_name=model_code, model_version = model_version,\n",
    "                         base_folder=base_folder, relative_input_folder=FileFinder.MLFOLDER_INPUT,\n",
    "                         relative_output_folder=FileFinder.MLFOLDER_PREDICT)\n",
    "\n",
    "full_path_in =  finder.get_full_input_file_name(input_file_name)  \n",
    "\n",
    "try:\n",
    "    print('Reading: ',full_path_in)\n",
    "    df = pd.read_csv(full_path_in)\n",
    "    display(df.head())\n",
    "\n",
    "    _,_,_,payroll_service, gcc, lcc, group, system, rest = finder.parse_input_file_name(input_file_name,include_remainder=True)\n",
    "    client, abkrs, period, other = rest.split('_')\n",
    "    ml_config = MLModelConfig.get_model_config_from_web_service_for_cust(ml_service, system=system,gcc=gcc,lcc=lcc,payroll_area=abkrs)\n",
    "except:\n",
    "    print('Error - file read not successful: ',full_path_in)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert input csv file to json ready for posting to prediction web service"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if use_csv_input:\n",
    "    print('Using csv input')\n",
    "    j_predict = pulled_df_to_json(df,ml_config,period,use_first_data_line_as_selection=True,use_value_title_format=True,clip_emps = num_emps_to_clip)\n",
    "\n",
    "    raw_data = {}\n",
    "    raw_data['data'] = j_predict\n",
    "\n",
    "    if save_csv_file_as_json:\n",
    "       predict_json_file_name =  input_file_name.split('.')[0] + '.json'\n",
    "       full_path_json = os.path.join(finder.get_output_folder(),predict_json_file_name)\n",
    "       print('Writing json predict in file: ', full_path_json)\n",
    "       with open(full_path_json, 'w') as outfile:\n",
    "           json.dump(raw_data, outfile, indent=4)\n",
    "           \n",
    "            \n",
    "else:        \n",
    "    import pprint\n",
    "    input_json_file_name = '' # <-- supply json file name here (including path)\n",
    "    print ('Using Json predict file: ',input_json_file_name)\n",
    "\n",
    "    with open(input_json_file_name) as json_data:\n",
    "            j_predict = json.load(json_data)\n",
    "\n",
    "print('Num emps: ',len(j_predict['values']))\n",
    "print('')\n",
    "print('First emp: ',j_predict['values'][0])\n",
    "print('')\n",
    "print('Selection: ',j_predict['selection'])\n",
    "print('')     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare headers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "headers = {\"Content-Type\":\"application/json\"}\n",
    "test_samples = json.dumps({'data':j_predict})\n",
    "test_samples = bytes(test_samples, encoding='utf8')        \n",
    "        \n",
    "print (json.dumps(j_predict['selection'], sort_keys=False, indent=4, separators=(',', ': ')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Call Prediction Web Service"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "response = requests.post(web_service_url, test_samples, headers=headers)  \n",
    "\n",
    "if response.status_code == 200:\n",
    "    json_predictions = response.json()\n",
    "    print('Prediction Web Service call successful - ' + str(len(json_predictions['Predictions'])) + ' emps processed' + ' Model used: ' + json_predictions['info']['azure_model_name'] + ' Model config ws status: ' + json_predictions['info']['config_web_service_call_status'])\n",
    "\n",
    "    print ('First 5 anomaly predictions: ')\n",
    "    for pred in json_predictions['Predictions'][:5]:\n",
    "        print('   Anomaly Score: ' + str(pred))\n",
    "else:\n",
    "    print('web service failed. Status: ' + str(response.status_code) + ' message: ' +  str(response.text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_predictions['info']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_predictions['selection']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_predictions['Predictions'][:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write json predictions to output file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_json_output_file_name = os.path.split(full_path_in)[-1]\n",
    "file_name_parts = predict_json_output_file_name.split('_')\n",
    "if file_name_parts[-1] == 'input.csv':\n",
    "    file_name_parts[-1] = 'predictions'\n",
    "else:\n",
    "    file_name_parts.append('predictions')\n",
    "predict_json_output_file_name = '_'.join(file_name_parts)  + '.json'\n",
    "full_path_json_out = os.path.join(finder.get_output_folder(),predict_json_output_file_name)\n",
    "print('writing prediction out: ',full_path_json_out)\n",
    "with open(full_path_json_out, 'w') as outfile:\n",
    "    json.dump(json_predictions, outfile, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyse predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = [x['Score'] for x in json_predictions['Predictions']]\n",
    "scores = scores[:num_emps_to_clip]\n",
    "scores[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "important_features = [x['Important_Features'] for x in json_predictions['Predictions']]\n",
    "important_features = important_features[:num_emps_to_clip]\n",
    "important_features[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clipped = df[:num_emps_to_clip].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clipped['Score'] = scores\n",
    "df_clipped.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_features = pd.DataFrame(important_features,columns=['LF1','LF2','LF3'])\n",
    "df_clipped = df_clipped.join(df_features)\n",
    "df_clipped.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Show input file with anomaly scores and important features appended (sorted by anomaly score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clipped_sorted = df_clipped.sort_values(by=['Score'])\n",
    "df_clipped_sorted.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
